{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FBHcaLdV_Ik2wP6WvQqqXG9PJ2GQ8uoh",
      "authorship_tag": "ABX9TyMMm9Dp82mYYS/Sq/N369U2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoraShruti/Duplicate-Quota-Questions/blob/main/Duplicate_Qoura_%2B_fe_%2B_dask_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_ouVqMQlkpB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dask.dataframe as dd\n",
        "!python -m pip install \"dask[distributed]\" --upgrade\n",
        "!pip install dask-ml\n",
        "from dask.distributed import Client, progress # importing Client and progress\n",
        "from dask_ml.feature_extraction.text import HashingVectorizer\n",
        "from dask_ml.wrappers import Incremental\n",
        "from dask_ml.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask-tfidf\n",
        "#from dask_ml.feature_extraction.text import TfidfTransformer\n"
      ],
      "metadata": {
        "id": "ACzyptDW_5VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up Dask Client\n",
        "csvfile = \"/content/drive/MyDrive/train.csv\"\n",
        "client = Client(n_workers=4)\n",
        "df = dd.read_csv(csvfile)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Z2VXSJlA1zvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import csv\n",
        "csvfile = \"/content/drive/MyDrive/train.csv\"\n",
        "df = pd.read_csv(csvfile)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "j_vmGazMmDhu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zTjJCTd1y2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "#df = df.rename(columns = {\"8200\\\"\":\"id\" , \"96804\": \"qid1\", \"177839\": \"qid2\", \"How do I known that someone viewed my Facebook profile?\":\"q1\", \"How do I know viewers of my Facebook profile?\":\"q2\", \"1\": \"label\"})"
      ],
      "metadata": {
        "id": "V7MJ7T5eovlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "id: not of much use\n"
      ],
      "metadata": {
        "id": "sWEx1SGZo6iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(30000,random_state=2)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "CH-jDLS_pQPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find missing values\n",
        "df.isnull().sum() # not much data missing"
      ],
      "metadata": {
        "id": "BWs89elvpUId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting rows that are completely duplicate duplicate\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "H_AeqR98pbjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of duplicate and non-duplicate questions\n",
        "print(df['is_duplicate'].value_counts())\n",
        "print((df['is_duplicate'].value_counts()/df['is_duplicate'].count())*100)\n",
        "df['is_duplicate'].value_counts().plot(kind='bar')\n",
        "\n",
        "#biased dataset more samples of non duplicate"
      ],
      "metadata": {
        "id": "-3sDv7Nlpiys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
        "print('Count of unique questions', np.unique(qid).shape[0])\n",
        "x = qid.value_counts()>1\n",
        "print('Number of questions getting repeated', x[x].shape[0])"
      ],
      "metadata": {
        "id": "LsdhVgELwETi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat ques histogram\n",
        "plt.hist(qid.value_counts().values, bins = 160)\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "giKipgTtscsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def common_words(row):\n",
        "  #\n",
        "  w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \"))) # set removes duplicate words\n",
        "  w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n",
        "  return len(w1 & w2) # intersection between the list of words in both questions\n",
        "def total_words(row):\n",
        "  #\n",
        "  w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \"))) # set removes duplicate words\n",
        "  w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n",
        "  return (len(w1) + len(w2)) # intersection between the list of words in both questions"
      ],
      "metadata": {
        "id": "-kav6O0FB400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "df['q1_len'] = df['question1'].str.len()\n",
        "df['q2_len'] = df['question2'].str.len()\n",
        "df['q1_len_w'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "df['q2_len_w'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "df['word_common'] = df.apply(common_words, axis=1)\n",
        "df['total_words'] = df.apply(total_words,axis=1)\n",
        "df['word_share'] = round(df['word_common']/df['total_words'],2) # roundoff to 2 decimal places\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZsDKzIVx-QNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of features - q1_len\n",
        "sns.displot(df['q1_len'])\n",
        "print(\"max of q1_len\",df['q1_len'].max())\n",
        "print(\"min of q1_len\",df['q1_len'].min())\n",
        "print(\"mean of q1_len\",df['q1_len'].mean())"
      ],
      "metadata": {
        "id": "QLQp53frowFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of features - q2_len\n",
        "sns.displot(df['q2_len'])\n",
        "print(\"max of q2_len\",df['q2_len'].max())\n",
        "print(\"min of q2_len\",df['q2_len'].min())\n",
        "print(\"mean of q2_len\",df['q2_len'].mean())"
      ],
      "metadata": {
        "id": "ipvYKIMVqH1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of features - q1_len_w\n",
        "sns.displot(df['q1_len_w'])\n",
        "print(\"max of q1_len_w\",df['q1_len_w'].max())\n",
        "print(\"min of q1_len_w\",df['q1_len_w'].min())\n",
        "print(\"mean of q1_len_W\",df['q1_len_w'].mean())"
      ],
      "metadata": {
        "id": "w2UsAegaqcmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of features - q2_len_w\n",
        "sns.displot(df['q2_len_w'])\n",
        "print(\"max of q2_len_w\",df['q2_len_w'].max())\n",
        "print(\"min of q2_len_w\",df['q2_len_w'].min())\n",
        "print(\"mean of q2_len_W\",df['q2_len_w'].mean())"
      ],
      "metadata": {
        "id": "3BDepjt-qx9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common words\n",
        "sns.histplot(df[df['is_duplicate'] == 0]['word_common'], label='non_duplicate') # can use distplot too\n",
        "sns.histplot(df[df['is_duplicate']== 1]['word_common'] , label = 'duplicate')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bc-jQKjerXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total words\n",
        "sns.histplot(df[df['is_duplicate'] == 0]['total_words'],label='non duplicate') # distplot is prettier\n",
        "sns.histplot(df[df['is_duplicate'] == 1]['total_words'],label='duplicate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nf3_LMsRscwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word share\n",
        "sns.histplot(df[df['is_duplicate'] == 0]['word_share'],label='non duplicate') #dist\n",
        "sns.histplot(df[df['is_duplicate'] == 1]['word_share'],label='duplicate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NMh_rzcCs9HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply bow to q1 n q2.\n",
        "# Considering is_duplicate column as y.\n",
        "# apply random forest algorithm\n",
        "\n",
        "ques_df = df[['question1','question2']]\n",
        "ques_df.head()\n",
        "final_df = df.drop(columns=['id','qid1','qid2','question1','question2'])\n",
        "print(final_df.shape)\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "jk7auamiss24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to find bow use count vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# merge to make a list of all questions\n",
        "# then use countvectorizer to get features\n",
        "questions = list(ques_new_df['question1']) + list(ques_new_df['question2']) #getting all the questions\n",
        "cv = CountVectorizer(max_features=3000) # limit num of features\n",
        "# now use these features to get bow for all ques\n",
        "# split this list of bow of all ques into 2 sets again -> q1 and q2\n",
        "q1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2) # transform to get bow"
      ],
      "metadata": {
        "id": "JmfhpKBr_QHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to df n concatenate\n",
        "temp_df1 = pd.DataFrame(q1_arr, index = ques_new_df.index)\n",
        "temp_df2 = pd.DataFrame(q2_arr, index = ques_new_df.index)\n",
        "temp_df = pd.concat([temp_df1, temp_df2], axis=1)\n",
        "temp_df.shape # sparse array"
      ],
      "metadata": {
        "id": "_lIPkw4WAEEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([final_df, temp_df], axis=1)\n",
        "print(final_df.shape)\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "8zDfQZpiAnyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(temp_df.iloc[:,0:-1].values, temp_df.iloc[:,-1].values, test_size=0.2, random_state =1)"
      ],
      "metadata": {
        "id": "Czex0nNZvGbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# applying random forest classifier\n",
        "rf = RandomForestClassifier() # the model\n",
        "rf.fit(X_train, y_train)    # train\n",
        "y_pred = rf.predict(X_test)  # test make predicttions on test data\n",
        "accuracy_score(y_test,  y_pred)    # check for accuracy"
      ],
      "metadata": {
        "id": "rnDEpwad4TiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "# checking accuracy with xgboost\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Vjy5y8NY4SSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}